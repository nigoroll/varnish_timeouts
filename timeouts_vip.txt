Ref https://github.com/varnishcache/varnish-cache/pull/2983

# Draft

## Consistent timeout naming

In the form of: ${subject}_${type}_timeout

Subjects:

- client (client session)
- backend (backend transaction)
- beresp
- resp
- pipe
- cli

We might want to consider new timeouts too, especially for req and bereq (see below).

Types:

- send
- fetch
- idle
- linger
- ...

## Mapping existing timeouts

In alphabetic order:

- backend_idle_timeout => no change
- between_bytes_timeout => beresp_read_timeout
- cli_timeout => cli_resp_timeout
- connect_timeout => backend_connect_timeout
- first_byte_timeout => beresp_start_timeout
- idle_send_timeout => resp_write_timeout, or replaced by self-tuning (see below)
- pipe_timeout => pipe_idle_timeout, or replaced by
  backend_idle_timeout (see below)
- send_timeout => resp_send_timeout
- thread_pool_timeout => no change
- timeout_idle => client_idle_timeout
- timeout_linger => client_linger_timeout

The goal besides consistent naming is to also increase clarity regarding
the role of each timeout, and make it easier for new timeouts to be added
in this model. It should also help better define how they relate to the
differences between http/1 and h2, or in broader terms http/1 and stream-based
protocols.

## New timeouts to consider

- bereq_send_timeout (wanted by both UPLEX and Varnish Software)
- req_fetch_timeout (wanted by both UPLEX and Varnish Software)
- beresp_fetch_timeout (Varnish Software use case, would be the equivalent
  of a last_byte_timeout if that existed)
- pipe_sess_timeout (Varnish Software use case)

New timeouts could have a default value of zero (no timeout) to maintain
existing behavior.

## Other considerations

Taken from past and current discussions.

### Issues with idle_send_timeout / resp_write_timeout

Current defaults:

- idle_send_timeout (old) / resp_write_timeout (new): 60s
- send_timeout (old) / resp_send_timeout (new): 600s

Current behavior:

send_timeout is our current and actual timeout from the user
perspective. idle_send_timeout is used to set SO_SNDTIMEO, so
individual writev() calls time out after that time.

Issues:

- idle_send_timeout is hard to understand and causes  confusion.

  The new name, resp_write_timeout, might help improve on that.

- What should it be set to?

  Users might ask themselves: what is a good value? Many users
  probably will not set it at all or set it to arbitrarily bad values.

  This is probably an indication that this timeout should be handled
  internally and not offer a knob hardly anyone knows how to push.

- Can extend send_timeout

  For the case of a write just before send_timeout, that actually gets
  extended by up to idle_send_timeout.

- Are we actually saving syscalls?

  We currently set SO_SNDTIMEOUT just once per session (if not updated
  in vcl). For the case where we actually reach send_timeout, we will
  retry (with the default values) a write 10 times.

  So we save syscalls for the fast path, yet not for the slow/error path.

Suggestion:

For the first three reasons given above, we suggest to drop
idle_send_timeout altogether. The timeout should be resp_send_timeout and
that's it.

A trivial approach would, before any write, calculate the remaining
time until resp_send_timeout is reached and set SO_SNDTIMEO accordingly.

This would add additional syscalls to the fast path, which we want to
avoid. Yet in order to stay within resp_send_timeout, we would need to
update SO_SNDTIMEO after the first write at the latest.

As a compromise, we suggest a divide-and-conquer strategy:

* Initially, set SO_SNDTIMEO to resp_send_timeout / 2 and remember
  a mono timestamp when that timeout would fire.

* Before each write, check if the time SO_SNDTIMEO would fire is past
  the deadline from resp_send_timeout and, if yes, set SO_SNDTIMEO
  half the remaining time again (with some lower bound at which
  halving is stopped, e.g. 1 second).

This way, the fast path will be unaffected and not include additional
syscalls, yet the divide-and-conquer strategy will result in less
syscalls overall than a fixed (low) SO_SNDTIMEO.

If we keep idle_send_timeout as resp_write_timeout we will also need more
"read" timeouts for req_fetch_timeout and bereq_send_timeout.

Because historically we have given control over between_bytes_timeout,
its successor beresp_read_timeout should remain.

### On pipe_idle_timeout (old: pipe_timeout)

pipe_idle_timeout is similar to backend_idle_timeout in that it
constitutes the maximum time to *wait for more data*, which is similar
to *wait for another request*.

Also, in pipe mode, backend_idle_timeout has no meaning.

Thus, it would make sense to only have one parameter for pipe /
non-pipe mode, which would be backend_idle_timeout as it is more
generic.

The counter-argument for having both is that changing
backend_idle_timeout requires setting it from vcl, while separate
parameters can more suitable defaults.

We thus suggest to keep both parameters despite the somehow
overlapping semantics.

### Expose all relevant timeouts to VCL

The title of this section doesn't leave much to say here, yet.

The VCL versions 4.0 and 4.1 need to support the established timeout names
as aliases to the new ones.

* XXX rename vcl sess.* -> client.* _or_ rename parameters client_* ->
  sess_*

* XXX backend_connect_timeout & backend_idle_timeout

  -> expose to vcl as backend.connect_timeout / backend.idle_timeout?
  -> or transfer ownership to directors?

* XXX fill in paramter list from gen.txt which we did not maintain
  properly anyway

We propose that everything except cli_resp_timeout and thread_pool_timeout are exposed
to VCL.

For client_*_timeout, would they ideally be accessible from a new vcl_sess subroutine?

### XXX ESI

TODO: Define how timeouts apply recursively to subrequests
