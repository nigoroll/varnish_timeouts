Ref https://github.com/varnishcache/varnish-cache/pull/2983

# Draft

## Consistent timeout naming

In the form of: ${subject}_${type}_timeout

Subjects:

- client (client session)
- backend (backend transaction)
- beresp
- resp
- pipe
- cli

We might want to consider new timeouts too, especially for req and bereq (see below).

Types:

- send
- fetch
- idle
- linger
- ...

## Mapping existing timeouts

In alphabetic order:

- backend_idle_timeout => no change
- between_bytes_timeout => beresp_idle_timeout
- cli_timeout => cli_resp_timeout
- connect_timeout => backend_connect_timeout
- first_byte_timeout => beresp_firstbyte_timeout (or beresp_start_timeout)
- idle_send_timeout => resp_idle_timeout, or replaced by self-tuning (see below)
- pipe_timeout => pipe_idle_timeout
- send_timeout => resp_send_timeout
- thread_pool_timeout => no change
- timeout_idle => client_idle_timeout
- timeout_linger => client_linger_timeout

The goal besides consistent naming is to also increase clarity regarding
the role of each timeout, and make it easier for new timeouts to be added
in this model. It should also help better define how they relate to the
differences between http/1 and h2, or in broader terms http/1 and stream-based
protocols.

## New timeouts to consider

- bereq_send_timeout (wanted by both UPLEX and Varnish Software)
- req_fetch_timeout (wanted by both UPLEX and Varnish Software)
- beresp_fetch_timeout (Varnish Software use case, would be the equivalent
  of a last_byte_timeout if that existed)
- pipe_sess_timeout (Varnish Software use case)

New timeouts could have a default value of zero (no timeout) to maintain
existing behavior.

## Other considerations

Taken from past and current discussions.

### Issues with idle_send_timeout

Current defaults:

- idle_send_timeout (old) / resp_idle_timeout (new): 60s
- send_timeout (old) / resp_send_timeout (new): 600s

Current behavior:

send_timeout is our current and actual timeout from the user
perspective. idle_send_timeout is used to set SO_SNDTIMEO, so
individual writev() calls time out after that time.

Issues:

- idle_send_timeout is hard to understand and causes  confusion.

  The new name, resp_idle_timeout, might help improve on that, yet a
  new clash might also be introduced with backend_idle_timeout /
  client_idle_timeout (which define the time to wait for another request)
  and pipe_idle_timeout (which defines the time to wait for more data).

- What should it be set to?

  Users might ask themselves: what is a good value? Many users
  probably will not set it at all or set it to arbitrarily bad values.

  This is probably an indication that this timeout should be handled
  internally and not offer a knob hardly anyone knows how to push.

- Can extend send_timeout

  For the case of a write just before send_timeout, that actually gets
  extended by up to idle_send_timeout.

- Are we actually saving syscalls?

  We currently set SO_SNDTIMEOUT just once per session (if not updated
  in vcl). For the case where we actually reach (resp_)?send_timeout,
  we will retry (with the default values) a write 10 times.

  So we save syscalls for the fast path, yet not for the slow/error path.

Suggestion:

For the first three reasons given above, we suggest to drop
idle_send_timeout altogether. The timeout should resp_send_timeout and
that's it.

A trivial approach would, before any write, calculate the remaining
time until resp_send_timeout is reached and set SO_SNDTIMEO accordingly.

This would add additional syscalls to the fast path, which we want to
avoid. Yet in order to stay within resp_send_timeout, we would need to
update SO_SNDTIMEO after the first write at the latest.

As a compromise, we suggest a divide-and-conquer strategy:

* Initially, set SO_SNDTIMEO to (resp_)?send_timeout / 2 and remember
  a mono timestamp when that timeout would fire.

* Before each write, check if the time SO_SNDTIMEO would fire is past
  the deadline from resp_send_timeout and, if yes, set SO_SNDTIMEO
  half the remaining time again (with some lower bound at which
  halving is stopped, e.g. 1 second).

This way, the fast path will be unaffected and not include additional
syscalls, yet the divide-and-conquer strategy will result in less
syscalls overall than a fixed (low) SO_SNDTIMEO.

If we keep idle_send_timeout as resp_idle_timeout we will also need more
"idle" timeouts for req_fetch_timeout and bereq_send_timeout.

Because historically we have given control over between_bytes_timeout,
its successor beresp_idle_timeout should remain.

### Expose all relevant timeouts to VCL

The title of this section doesn't leave much to say here, yet.

The VCL versions 4.0 and 4.1 need to support the established timeout names
as aliases to the new ones.

Below is the complete list of suggested timeouts:

- backend_connect_timeout
- backend_idle_timeout 
- backend_task_timeout
- bereq_send_timeout
- beresp_firstbyte_timeout
- beresp_idle_timeout
- beresp_fetch_timeout
- client_idle_timeout
- client_linger_timeout
- client_task_timeout
- cli_resp_timeout
- pipe_idle_timeout
- pipe_sess_timeout
- req_fetch_timeout
- resp_send_timeout
- thread_pool_timeout

We propose that everything except cli_resp_timeout and thread_pool_timeout are exposed
to VCL.

For client_*_timeout, would they ideally be accessible from a new vcl_sess subroutine?

### XXX ESI

TODO: Define how timeouts apply recursively to subrequests
