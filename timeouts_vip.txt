Ref https://github.com/varnishcache/varnish-cache/pull/2983

# Draft

## Consistent timeout naming

In the form of: ${subject}_${type}_timeout

Subjects:

- client (client session)
- backend (backend transaction)
- beresp
- resp
- pipe
- cli

We might want to consider new timeouts too, especially for req and bereq (see below).

Types:

- send
- fetch
- idle
- linger
- ...

## Mapping existing timeouts

In alphabetic order:

- backend_idle_timeout => no change
- between_bytes_timeout => beresp_idle_timeout
- cli_timeout => cli_resp_timeout
- connect_timeout => backend_connect_timeout
- first_byte_timeout => beresp_firstbyte_timeout
- idle_send_timeout => resp_idle_timeout, or replaced by self-tuning (see below)
- pipe_timeout => pipe_idle_timeout
- send_timeout => resp_send_timeout
- thread_pool_timeout => no change
- timeout_idle => client_idle_timeout
- timeout_linger => client_linger_timeout

The goal besides consistent naming is to also increase clarity regarding the role of each timeout, and make it easier for new timeouts to be added in this model. It should also help better define how they relate to the differences between http/1 and h2, or in broader terms http/1 and stream-based protocols.

## New timeouts to consider

- bereq_send_timeout (suggested by slink. dridi: can slow clients DoS a backend with request bodies today? dridi: the answer is yes)
- req_fetch_timeout (for std.cache_req_body? getting the whole request on the client side dridi: may also affect the fetch, see above (for no cache_req_body (yes)))
- beresp_fetch_timeout (VS end-user case)
- pipe_sess_timeout (VS end-user case)

New timeouts should have a default value of zero (no timeout) to maintain existing behavior.

## Other considerations

Taken from past and current discussions.

### issues with idle_send_timeout (old name) / resp_idle_timeout (new name)

current defaults:

- idle_send_timeout (old) / resp_idle_timeout (new) : 60s
- send_timeout (old) / resp_send_timeout (new): 600s

current behavior:

(resp_)?send_timeout is our actual timeout from the user
perspective. idle_send_timeout/resp_idle_timeout is used to set
SO_SNDTIMEO, so individual writev() calls time out after that time.

issues:

- idle_send_timeout/resp_idle_timeout is hard to understand and causes
  confusion.

  The new names might help improve on that, yet a new clash might also
  be introduced with backend_idle_timeout / client_idle_timeout (which
  define the time to wait for another request) and pipe_idle_timeout
  (which defines the time to wait for more data).

- what should it be set to?

  Users might ask themselves: what is a good value? Many users
  probably will not set it at all or set it to arbitrarily bad values.

- can extend (resp_)?send_timeout

  for the case of a write just before (resp_)?send_timeout, that
  actually gets extended by idle_send_timeout/resp_idle_timeout

- are we actually saving syscalls?

  We currently set SO_SNDTIMEOUT just once per session (if not updated
  in vcl). For the case where we actually reach (resp_)?send_timeout,
  we will retry (with the default values) a write 10 times.

  So we save syscalls for the fast path, yet not for the slow/error path.

suggestion:

For the first three reasons given above, we suggest to drop
idle_send_timeout/resp_idle_timeout altogether. The timeout should
(resp_)?send_timeout and that's it.

A trivial approach would, before any write, calculate the remaining
time until (resp_)?send_timeout is reached and set SO_SNDTIMEO
accordingly.

This would add additional syscalls to the fast path, which we want to
avoid. Yet in order to stay within (resp_)?send_timeout, we would need
to update SO_SNDTIMEO after the first write at the latest.

As a compromise, we suggest a divide-and-conquer strategy:

* initially, set SO_SNDTIMEO to (resp_)?send_timeout / 2 and remember
  a mono timestamp when that timeout would fire

* before each write, check if the time SO_SNDTIMEO would fire is past
  the deadline from (resp_)?send_timeout and, if yes, set SO_SNDTIMEO
  half the remaining time again (with some lower bound at which
  halving is stopped, e.g. 1 second).

This way, the fast path will be unaffected and not include additional
syscalls, yet the divide-and-conquer strategy will result in less
syscalls overall than a fixed (low) SO_SNDTIMEO.

### Expose all relevant timeouts to VCL

The title of this section doesn't leave much to say here, yet.

The VCL versions 4.0 and 4.1 need to support the established timeout names as aliases to the new ones.

Timeouts to expose:


- backend_connect_timeout
- backend_idle_timeout 
- backend_task_timeout
- bereq_send_timeout
- beresp_firstbyte_timeout
- beresp_idle_timeout
- beresp_fetch_timeout
- client_idle_timeout
- client_linger_timeout
- client_task_timeout
- cli_resp_timeout => no
- pipe_idle_timeout
- pipe_sess_timeout
- req_fetch_timeout
- resp_idle_timeout
- resp_send_timeout
- thread_pool_timeout => no

In other words, everything except cli_resp_timeout and thread_pool_timeout.

For client_*_timeout, would they ideally be accessible from a new vcl_sess subroutine?

### XXX ESI

TODO: Define how timeouts apply recursively to subrequests
